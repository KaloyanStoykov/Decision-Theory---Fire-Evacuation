{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0aefad",
   "metadata": {},
   "source": [
    "# Firefighter Game - MDP and Q-Learning\n",
    "\n",
    "## Research Question\n",
    "\n",
    "**Which algorithm is more efficient for solving the firefighter game? Classical algorithm or reinforcement learning.**\n",
    "\n",
    "The algorithms will be assesed on:\n",
    " - Steps to reach the target\n",
    " - Amount of deaths\n",
    " - Illegal actions\n",
    " - Total rewards\n",
    "\n",
    "## The Environment\n",
    "\n",
    "We used gymnasium as the execution environment for both algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1952daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "gym.envs.registration.register(\n",
    "    id=\"FireFighterWorld\",\n",
    "    entry_point=\"envs:FireFighterWorld\",\n",
    ")\n",
    "\n",
    "env = gym.make(\"FireFighterWorld\", render_mode=\"human\" if RENDER else \"rgb_array\")\n",
    "env.reset(seed=42) # The seed is set to keep the results consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ebb78",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "How it all works together - since the codebase is easily spread around in chunks of relevant logic, we can play around with the rendering and/or configuration to tweak small changes around the game while still keeping our models intact and untouched\n",
    "\n",
    "The `.png` files can be changed for different structures to change how the game looks and feels.\n",
    "# MDP\n",
    "Summary:\n",
    "The basic idea of the implementation here is to realize a classical MDP-based agent which operates in a stochastic (random) environment to achieve a desired action -> end state. In our case:\n",
    "- We have a building layout (grid) with setup obstacles (walls) and a strategy for a fire_spread being a static_fire for now.\n",
    "  - Our agent has a simple objective - reach the cat in order for the episode to finish successfully\n",
    "\n",
    "MDP Characteristics:\n",
    "In order to model a theoretically correct Markov Decision Process (MDP) we would need to have:\n",
    "- State Representation - agent is at a location on a (x,y) basis.\n",
    "- Actions Choice - a set of allowed actions - up, down, left, right.\n",
    "- Transition model - agent needs to act in a stochastic environment. Our implementation has introduced a \"stochastic\" environment - given that a optimal action is chosen, it is not 100% defined that this action will be actually executed, much like in real life.\n",
    "- Reward function - calculated rewards for each position. Stepping in fire -> -100 and episode ends, else accumulate reward.\n",
    "- **Processes that are MARKOVIAN** - ending up in state x1 must solely depend on the current state x0 and action a0. No history is memorized and taken into account for next state. P(s`| s, a)\n",
    "- Value iteration - finds optimal policy and update value of each state and expected return for each action from that state\n",
    "\n",
    "\n",
    "## How it's connected to the gym environment\n",
    "\n",
    "\n",
    "## Advantages:\n",
    "- **Challenging to start, easier to understand**: If the basic topics of MDP are learnt and some experience with Python is there, modelling the MDP is straightforward, although still challenging at some points\n",
    "- **Debug and Validate**: Easy validation due to the static environment and easier debugging\n",
    "- **Start from an MDP**: A good start point ot extend further for different methods and purposes\n",
    "## Disadvantages:\n",
    "- **Lacks realism**: Although, variables and logic can be changed to initialize different fire configuration, a static environment would not be easily found in a real-life scenario especially for a fire-spread mechanism\n",
    "- **Continuous reaction**: Apart from a random action that might lead an agent to step in a fire, it doesnt actually need to account for any unexpected change.\n",
    "\n",
    "\n",
    "# Q-learning\n",
    "\n",
    "# Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13ea05-1b3e-44d9-bc77-797b7bcb6f24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
