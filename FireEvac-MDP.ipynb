{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9647308d-61f7-4195-937c-2cff38810c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from envs.grid import Grid\n",
    "from envs.constants import SQUARE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13a716-a97c-4775-853d-ca2ab6f67bb5",
   "metadata": {},
   "source": [
    "# Fire Evacuation Planner MDP\n",
    "This agent will implement a classic MDP with states, rewards and transition models\n",
    "Extending the MDP to our use case could include:\n",
    " - Fire Spread algorithm:\n",
    "   - Episode ends if stepping in fire state\n",
    "   - Firefighter (MDP agent) recieves reward for steps that have people needing to rescue\n",
    "   - Generate an environment that includes more sophisticated properties - generate walls, based on grid, doors and so on...\n",
    "   - Default reward could be something like -0.04 to encourage efficiency\n",
    "   - Pass arguments to the grid when defining the base environment (walls, starting fire, people)\n",
    "\n",
    "## Compare Reinforcement Learning Methods (Q-learning, SARSA) to Classical Methods (Policy iteration, value iteration, linear programming)\n",
    "The separate models will aim to answer whether classical models or RL-based are better suited for a simulation of a real-world fire hazard on a building floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990c01c-7f93-4805-a24e-f646b6b04227",
   "metadata": {},
   "source": [
    "# Possible challenges of a classical MDP implementation\n",
    "Since we are dealing with a classical-based MDP, we would need to make sure that all processes are markovian - taking action based only on current state and possible rewards.\n",
    "\n",
    "If we encode the fire in a way that it spreads independently, then that would mean that our agent acts in a non-MDP way.\n",
    "\n",
    " - One way to solve this would be to include the fire status of every grid, which can quickly turn out to be alot of calculations and statuses for a simple grid.\n",
    "\n",
    "For small grids in examples like 3x4 size, this would be a challenge but for bigger ones, Reinforcement Learning almost definitely need to be adopted in order to manage the changing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2335576-98d0-42fe-8ad2-e480e75a7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.grid import Grid\n",
    "from envs.tiles.tile import Tile # Import Tile for type hinting if necessary\n",
    "\n",
    "class FireEvacuationAgentMDP:\n",
    "    def __init__(self, start_state: tuple[int, int], grid: Grid):\n",
    "        self.grid = grid\n",
    "        self.rows = grid.size\n",
    "        self.cols = grid.size\n",
    "        self.actions = ['up', 'left', 'right', 'down']\n",
    "\n",
    "        self.possible_agent_positions = []\n",
    "        for y in range(self.rows):\n",
    "            for x in range(self.cols):\n",
    "                if self.grid.tiles[x][y].is_traversable:\n",
    "                    self.possible_agent_positions.append((x, y))\n",
    "\n",
    "        if start_state in self.possible_agent_positions:\n",
    "            self.start_state_position = self.current_state_position = start_state\n",
    "        else:\n",
    "            if self.possible_agent_positions:\n",
    "                self.start_state_position = self.current_state_position = self.possible_agent_positions[0]\n",
    "            else:\n",
    "                self.start_state_position = self.current_state_position = (0, 0) # Fallback\n",
    "\n",
    "        # The actual MDP state will be (agent_x, agent_y, fire_config_tuple)\n",
    "        self.current_mdp_state = self._get_current_mdp_state()\n",
    "\n",
    "    def _get_fire_config_tuple(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Returns a flattened tuple representing the fire status of all traversable tiles.\n",
    "        \"\"\"\n",
    "        fire_status = []\n",
    "        for y in range(self.rows):\n",
    "            for x in range(self.cols):\n",
    "                if self.grid.tiles[x][y].is_traversable:\n",
    "                    fire_status.append(self.grid.tiles[x][y].is_on_fire)\n",
    "        return tuple(fire_status)\n",
    "\n",
    "    def _get_current_mdp_state(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Combines agent's position and fire configuration into the full MDP state.\n",
    "        \"\"\"\n",
    "        return (self.current_state_position[0], self.current_state_position[1]) + self._get_fire_config_tuple()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state_position = self.start_state_position\n",
    "        # When resetting, the fire state should also typically reset for MDPs,\n",
    "        self.current_mdp_state = self._get_current_mdp_state()\n",
    "        return self.current_mdp_state\n",
    "\n",
    "    def step(self, action: str) -> tuple:\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        old_state_position = self.current_state_position\n",
    "        x, y = self.current_state_position\n",
    "        \n",
    "        new_state_position = None\n",
    "        match action:\n",
    "            case 'up':\n",
    "                new_state_position = (x, y + 1)\n",
    "            case 'left':\n",
    "                new_state_position = (x - 1, y)\n",
    "            case 'right':\n",
    "                new_state_position = (x + 1, y)\n",
    "            case 'down':\n",
    "                new_state_position = (x, y - 1)\n",
    "        \n",
    "        # Check if new_state_position is within bounds and traversable\n",
    "        if (0 <= new_state_position[0] < self.cols and\n",
    "            0 <= new_state_position[1] < self.rows and\n",
    "            self.grid.tiles[new_state_position[0]][new_state_position[1]].is_traversable):\n",
    "            self.current_state_position = new_state_position\n",
    "        else:\n",
    "            self.current_state_position = old_state_position\n",
    "\n",
    "        self.grid.update() # This is where the fire spread/extinguish logic runs\n",
    "\n",
    "        reward = 0 # To be defined later\n",
    "\n",
    "        is_terminal = False # To be defined later\n",
    "\n",
    "        self.current_mdp_state = self._get_current_mdp_state()\n",
    "\n",
    "        return self.current_mdp_state, reward, is_terminal, {} # info dict for additional details\n",
    "\n",
    "    def get_possible_states(self):\n",
    "        # For a full MDP, this would ideally return all (agent_pos, fire_config) combinations,\n",
    "        # which can be extremely large. For now, it returns only possible agent positions.\n",
    "        # This highlights the challenge of explicitly listing all states for a dynamic MDP.\n",
    "        return self.possible_agent_positions\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Agent is at state {self.current_mdp_state}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba75e5f-ef43-4241-a0c7-32f9f8b8c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_instance = Grid(size = 5, tile_size=SQUARE_SIZE)\n",
    "initial_agent_pos = (0,0)\n",
    "found_traversable = False\n",
    "for y in range(grid_instance.size):\n",
    "    for x in range(grid_instance.size):\n",
    "        if grid_instance.tiles[x][y].is_traversable:\n",
    "            actual_start_state = (x, y)\n",
    "            found_traversable = True\n",
    "            break\n",
    "    if found_traversable:\n",
    "        break\n",
    "\n",
    "\n",
    "if not found_traversable:\n",
    "    print(\"Warning: No traversable states found in the grid!\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c9c32ad-197e-4ddf-aea9-d36d56f48179",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = FireEvacuationAgentMDP(start_state=actual_start_state, grid=grid_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fed7f005-2c2b-4bb7-82f5-bea8579cd44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is at state (0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d08616c-5540-4677-a2ec-834b424404c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is at state (1, 0)\n",
      "Agent is at state (1, 0)\n",
      "Agent is at state (0, 0)\n",
      "Agent is at state (0, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test moves\n",
    "mdp.step('right')\n",
    "print(mdp)\n",
    "mdp.step('down')\n",
    "print(mdp)\n",
    "mdp.step('left')\n",
    "print(mdp)\n",
    "mdp.step('up')\n",
    "print(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd045a16-fa0d-4197-bccc-6ca97047441b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
