{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9647308d-61f7-4195-937c-2cff38810c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from envs.grid import Grid\n",
    "from envs.constants import SQUARE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13a716-a97c-4775-853d-ca2ab6f67bb5",
   "metadata": {},
   "source": [
    "# Fire Evacuation Planner MDP\n",
    "This agent will implement a classic MDP with states, rewards and transition models\n",
    "Extending the MDP to our use case could include:\n",
    " - Fire Spread algorithm:\n",
    "   - Episode ends if stepping in fire state\n",
    "   - Firefighter (MDP agent) recieves reward for steps that have people needing to rescue\n",
    "   - Generate an environment that includes more sophisticated properties - generate walls, based on grid, doors and so on...\n",
    "   - Default reward could be something like -0.04 to encourage efficiency\n",
    "   - Pass arguments to the grid when defining the base environment (walls, starting fire, people)\n",
    "\n",
    "## Compare Reinforcement Learning Methods (Q-learning, SARSA) to Classical Methods (Policy iteration, value iteration, linear programming)\n",
    "The separate models will aim to answer whether classical models or RL-based are better suited for a simulation of a real-world fire hazard on a building floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990c01c-7f93-4805-a24e-f646b6b04227",
   "metadata": {},
   "source": [
    "# Possible challenges of a classical MDP implementation\n",
    "Since we are dealing with a classical-based MDP, we would need to make sure that all processes are markovian - taking action based only on current state and possible rewards.\n",
    "\n",
    "If we encode the fire in a way that it spreads independently, then that would mean that our agent acts in a non-MDP way.\n",
    "\n",
    " - One way to solve this would be to include the fire status of every grid, which can quickly turn out to be alot of calculations and statuses for a simple grid.\n",
    "\n",
    "For small grids in examples like 3x4 size, this would be a challenge but for bigger ones, Reinforcement Learning almost definitely need to be adopted in order to manage the changing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2335576-98d0-42fe-8ad2-e480e75a7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from envs.grid import Grid\n",
    "from envs.tiles.tile import Tile\n",
    "from envs.constants import Action, GRID_SIZE # Import GRID_SIZE\n",
    "\n",
    "class FireEvacuationAgentMDP:\n",
    "    def __init__(self, np_random: np.random.RandomState):\n",
    "        \"\"\"\n",
    "        Initializes the FireEvacuationAgentMDP.\n",
    "        The Grid itself manages the agent's initial position and fire state.\n",
    "        \n",
    "        Args:\n",
    "            np_random: A NumPy random state object for reproducibility within the MDP and Grid.\n",
    "        \"\"\"\n",
    "        self.np_random = np_random\n",
    "        self.grid = Grid(self.np_random) # Pass np_random to the Grid constructor\n",
    "        \n",
    "        # Grid dimensions are now derived from GRID_SIZE in constants.py\n",
    "        self.rows = GRID_SIZE\n",
    "        self.cols = GRID_SIZE\n",
    "        \n",
    "        # Actions are strings based on the Action enum\n",
    "        self.actions = [action.name.lower() for action in Action if action.name.lower() in ['up', 'left', 'right', 'down']] \n",
    "        # Only include movement actions for now, as PickPerson, BreakDoor, PutOutFire require more complex logic\n",
    "        # for a classic MDP state representation and will be added later if necessary.\n",
    "\n",
    "        # Determine all possible agent positions based on traversable tiles in the grid\n",
    "        self.possible_agent_positions = []\n",
    "        for y in range(self.rows):\n",
    "            for x in range(self.cols):\n",
    "                if self.grid.tiles[x][y].is_traversable:\n",
    "                    self.possible_agent_positions.append((x, y))\n",
    "\n",
    "        # The agent's actual starting position is determined by the Grid's initialization\n",
    "        self.start_state_position = tuple(self.grid.agent.pos) # Convert np.array to tuple\n",
    "        self.current_state_position = tuple(self.grid.agent.pos) # Agent's current (x,y) position\n",
    "\n",
    "        # The actual MDP state combines agent's position and fire configuration\n",
    "        self.current_mdp_state = self._get_current_mdp_state()\n",
    "\n",
    "    def _get_fire_config_tuple(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Returns a flattened tuple representing the fire status of all traversable tiles.\n",
    "        \"\"\"\n",
    "        fire_status = []\n",
    "        for y in range(self.rows):\n",
    "            for x in range(self.cols):\n",
    "                # Ensure we only check tiles that exist in the grid (e.g., not None if grid creation is partial)\n",
    "                tile = self.grid.tiles[x][y]\n",
    "                if tile and tile.is_traversable: # Only include traversable tiles in the state representation\n",
    "                    fire_status.append(tile.is_on_fire)\n",
    "        return tuple(fire_status)\n",
    "\n",
    "    def _get_current_mdp_state(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Combines agent's position and fire configuration into the full MDP state.\n",
    "        \"\"\"\n",
    "        return (self.current_state_position[0], self.current_state_position[1]) + self._get_fire_config_tuple()\n",
    "\n",
    "    def reset(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Resets the MDP to a new initial state by recreating the Grid.\n",
    "        Returns the new initial MDP state.\n",
    "        \"\"\"\n",
    "        self.grid = Grid(self.np_random) # Recreate grid to reset agent, cat, and fire\n",
    "        self.current_state_position = tuple(self.grid.agent.pos) # Get new agent position from reset grid\n",
    "        self.start_state_position = tuple(self.grid.agent.pos) # Update start state as well\n",
    "        self.current_mdp_state = self._get_current_mdp_state()\n",
    "        return self.current_mdp_state\n",
    "\n",
    "    def step(self, action_str: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Executes an action in the environment.\n",
    "        \n",
    "        Args:\n",
    "            action_str (str): The string name of the action (e.g., 'up', 'down').\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (new_mdp_state, reward, is_terminal, info)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert the incoming string action to its corresponding Action enum member\n",
    "            action_enum = Action[action_str.upper()]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Invalid action: {action_str}. Must be one of {[a.name.lower() for a in Action]}\")\n",
    "\n",
    "        # Determine the intended next position based on the action\n",
    "        x, y = self.current_state_position\n",
    "        intended_next_position = None\n",
    "        match action_enum:\n",
    "            case Action.UP:\n",
    "                intended_next_position = (x, y + 1)\n",
    "            case Action.LEFT:\n",
    "                intended_next_position = (x - 1, y)\n",
    "            case Action.RIGHT:\n",
    "                intended_next_position = (x + 1, y)\n",
    "            case Action.DOWN:\n",
    "                intended_next_position = (x, y - 1)\n",
    "            case _: # Handle other actions if they were included in self.actions\n",
    "                # For now, we only handle movement, others will be no-ops or raise error\n",
    "                intended_next_position = (x, y) # Stay in place for non-movement actions or invalid ones in match\n",
    "\n",
    "        # Call the Grid's update method, which handles agent movement and fire spread\n",
    "        # The Grid's update method itself determines if the agent actually moved or stayed.\n",
    "        # It returns True if agent moved, False if blocked.\n",
    "        agent_moved = self.grid.update(intended_next_position)\n",
    "\n",
    "        # Update the MDP's internal agent position based on the Grid's actual agent position\n",
    "        self.current_state_position = tuple(self.grid.agent.pos)\n",
    "\n",
    "        # Determine reward (placeholder for now)\n",
    "        reward = 0\n",
    "\n",
    "        # Determine if terminal state (placeholder for now)\n",
    "        # Check if agent is on fire *after* the grid update\n",
    "        is_terminal = self.grid.is_agent_dead() # Or if cat is rescued, etc.\n",
    "\n",
    "        # Construct the full new MDP state after all environment dynamics\n",
    "        self.current_mdp_state = self._get_current_mdp_state()\n",
    "\n",
    "        info = {\"agent_moved\": agent_moved} # Provide some info about the move outcome\n",
    "        return self.current_mdp_state, reward, is_terminal, info\n",
    "\n",
    "    def get_possible_states(self) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        Returns a list of all possible (x,y) agent positions.\n",
    "        Note: For a full MDP state, this would need to generate all\n",
    "        (agent_pos, fire_config) combinations, which is generally intractable.\n",
    "        This method refers to traversable (x,y) coordinates.\n",
    "        \"\"\"\n",
    "        return self.possible_agent_positions\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Provides a string representation of the current MDP state.\n",
    "        \"\"\"\n",
    "        agent_pos_str = f\"Agent Pos: {self.current_state_position}\"\n",
    "        fire_config_str = f\"Fire Config: {self._get_fire_config_tuple()}\"\n",
    "        return f\"Current MDP State: ({agent_pos_str}, {fire_config_str})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd045a16-fa0d-4197-bccc-6ca97047441b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing FireEvacuationAgentMDP for a 6x6 grid...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FireFighter' object has no attribute 'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 2. Initialize the FireEvacuationAgentMDP\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# It now takes np_random_instance, and the Grid is created internally within the MDP.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitializing FireEvacuationAgentMDP for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGRID_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGRID_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m grid...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m mdp = FireEvacuationAgentMDP(np_random=np_random_instance)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFireEvacuationAgentMDP initialized.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Display initial state\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mFireEvacuationAgentMDP.__init__\u001b[39m\u001b[34m(self, np_random)\u001b[39m\n\u001b[32m     32\u001b[39m             \u001b[38;5;28mself\u001b[39m.possible_agent_positions.append((x, y))\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# The agent's actual starting position is determined by the Grid's initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28mself\u001b[39m.start_state_position = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m.grid.agent.pos) \u001b[38;5;66;03m# Convert np.array to tuple\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mself\u001b[39m.current_state_position = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m.grid.agent.pos) \u001b[38;5;66;03m# Agent's current (x,y) position\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# The actual MDP state combines agent's position and fire configuration\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'FireFighter' object has no attribute 'pos'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Ensure these imports match your file structure relative to where you run this script\n",
    "# Assuming FireEvacuationAgentMDP.py is in the same directory as this test script\n",
    "from envs.constants import Action, GRID_SIZE # Import GRID_SIZE for context\n",
    "\n",
    "# 1. Initialize a NumPy random state (crucial for Grid and MDP consistency)\n",
    "seed = 42 # For reproducibility\n",
    "np_random_instance = np.random.RandomState(seed)\n",
    "\n",
    "# 2. Initialize the FireEvacuationAgentMDP\n",
    "# It now takes np_random_instance, and the Grid is created internally within the MDP.\n",
    "print(f\"Initializing FireEvacuationAgentMDP for a {GRID_SIZE}x{GRID_SIZE} grid...\")\n",
    "mdp = FireEvacuationAgentMDP(np_random=np_random_instance)\n",
    "print(\"FireEvacuationAgentMDP initialized.\")\n",
    "\n",
    "# Display initial state\n",
    "print(f\"\\nInitial MDP state: {mdp.current_mdp_state}\")\n",
    "print(f\"Agent's starting position (from grid): {mdp.current_state_position}\")\n",
    "print(f\"Possible actions: {mdp.actions}\")\n",
    "\n",
    "# 3. Test some steps\n",
    "print(\"\\n--- Testing Agent Steps ---\")\n",
    "\n",
    "# Step 1: Move right\n",
    "action_to_take = Action.RIGHT.name.lower()\n",
    "print(f\"\\nAgent takes action: {action_to_take}\")\n",
    "new_state, reward, is_terminal, info = mdp.step(action_to_take)\n",
    "print(f\"New MDP state: {new_state}\")\n",
    "print(f\"Agent position after move: {mdp.current_state_position}\")\n",
    "print(f\"Reward: {reward}, Terminal: {is_terminal}\")\n",
    "print(f\"Info (Agent moved): {info['agent_moved']}\")\n",
    "\n",
    "\n",
    "# Step 2: Move down\n",
    "action_to_take = Action.DOWN.name.lower()\n",
    "print(f\"\\nAgent takes action: {action_to_take}\")\n",
    "new_state, reward, is_terminal, info = mdp.step(action_to_take)\n",
    "print(f\"New MDP state: {new_state}\")\n",
    "print(f\"Agent position after move: {mdp.current_state_position}\")\n",
    "print(f\"Reward: {reward}, Terminal: {is_terminal}\")\n",
    "print(f\"Info (Agent moved): {info['agent_moved']}\")\n",
    "\n",
    "# Step 3: Try to move into a wall\n",
    "# Your Grid's `_create_walls` method places walls at specific positions.\n",
    "# The Grid's `update` method will prevent movement into non-traversable tiles.\n",
    "# To properly test this, we might need to know the initial random agent position and wall positions.\n",
    "# Let's try an action that *might* lead to a wall based on GRID_SIZE and typical wall placements.\n",
    "# For example, if agent starts at (0,0) and tries to move left, it will hit a boundary.\n",
    "# If GRID_SIZE is 5, and there's a wall at (0,2), moving up from (0,1) would hit it.\n",
    "\n",
    "# Let's try to ensure the agent is at (0,0) for a boundary test, if the current Grid allows it.\n",
    "# Note: Since the Grid randomly places the agent, this specific test might not always\n",
    "# put the agent in a position to hit a wall directly without manual adjustment or\n",
    "# a more sophisticated test setup.\n",
    "print(f\"\\nAttempting a boundary/wall test:\")\n",
    "current_agent_pos_for_test = mdp.current_state_position\n",
    "print(f\"Agent starts this test at: {current_agent_pos_for_test}\")\n",
    "\n",
    "# Try moving left (if agent is at x=0, this hits boundary)\n",
    "action_to_take_wall = Action.LEFT.name.lower()\n",
    "print(f\"Agent attempts action: {action_to_take_wall}\")\n",
    "new_state, reward, is_terminal, info = mdp.step(action_to_take_wall)\n",
    "print(f\"New MDP state: {new_state}\")\n",
    "print(f\"Agent position after attempt: {mdp.current_state_position}\")\n",
    "print(f\"Info (Agent moved): {info['agent_moved']}\")\n",
    "if not info['agent_moved']:\n",
    "    print(\"Agent was blocked (boundary or obstacle) and stayed in place.\")\n",
    "\n",
    "# 4. Observe fire dynamics over several steps\n",
    "print(\"\\n--- Observing Fire Dynamics ---\")\n",
    "# Reset the MDP to a new random start to observe fire changes.\n",
    "# Each reset will create a new random grid with new agent/cat positions and fire states.\n",
    "initial_mdp_state_after_reset = mdp.reset()\n",
    "initial_fire_config = initial_mdp_state_after_reset[2:]\n",
    "print(f\"\\nInitial state after reset: {initial_mdp_state_after_reset}\")\n",
    "print(f\"Initial fire configuration after reset: {initial_fire_config}\")\n",
    "\n",
    "# Take several 'noop' steps (e.g., trying to move into a wall or just staying put)\n",
    "# to allow fire to spread/extinguish, as grid.update() is called each step.\n",
    "print(\"\\nTaking 5 steps to observe fire spread...\")\n",
    "for i in range(5):\n",
    "    # Agent tries to move right. This ensures grid.update() runs.\n",
    "    # If the agent is blocked, it effectively becomes a 'noop' for position.\n",
    "    action_to_take_fire_obs = Action.RIGHT.name.lower() \n",
    "    new_state, reward, is_terminal, info = mdp.step(action_to_take_fire_obs)\n",
    "    \n",
    "    current_fire_config = new_state[2:]\n",
    "    fire_changed = \"YES\" if current_fire_config != initial_fire_config else \"NO\"\n",
    "    print(f\"Step {i+1}: Agent Pos: {mdp.current_state_position}, Fire changed: {fire_changed}, New fire config: {current_fire_config}\")\n",
    "    initial_fire_config = current_fire_config # Update for next comparison\n",
    "\n",
    "    if is_terminal:\n",
    "        print(f\"Agent became terminal at step {i+1} (e.g., died in fire).\")\n",
    "        break # Stop if a terminal state is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b73c4-c069-4ec3-bb06-c58885d64ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
