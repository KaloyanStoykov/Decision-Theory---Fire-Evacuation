{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9647308d-61f7-4195-937c-2cff38810c5d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from envs.grid import Grid\n",
    "import time\n",
    "from envs.ui.sprites import load_srpite_map\n",
    "\n",
    "load_srpite_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13a716-a97c-4775-853d-ca2ab6f67bb5",
   "metadata": {},
   "source": [
    "# Fire Evacuation Planner MDP\n",
    "This agent will implement a classic MDP with states, rewards and transition models\n",
    "Extending the MDP to our use case could include:\n",
    " - Fire Spread algorithm:\n",
    "   - Episode ends if stepping in fire state\n",
    "   - Firefighter (MDP agent) recieves reward for steps that have people needing to rescue\n",
    "   - Generate an environment that includes more sophisticated properties - generate walls, based on grid, doors and so on...\n",
    "   - Default reward could be something like -0.04 to encourage efficiency\n",
    "   - Pass arguments to the grid when defining the base environment (walls, starting fire, people)\n",
    "\n",
    "## Compare Reinforcement Learning Methods (Q-learning, SARSA) to Classical Methods (Policy iteration, value iteration, linear programming)\n",
    "The separate models will aim to answer whether classical models or RL-based are better suited for a simulation of a real-world fire hazard on a building floor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990c01c-7f93-4805-a24e-f646b6b04227",
   "metadata": {},
   "source": [
    "# Possible challenges of a classical MDP implementation\n",
    "Since we are dealing with a classical-based MDP, we would need to make sure that all processes are markovian - taking action based only on current state and possible rewards.\n",
    "\n",
    "If we encode the fire in a way that it spreads independently, then that would mean that our agent acts in a non-MDP way.\n",
    "\n",
    " - One way to solve this would be to include the fire status of every grid, which can quickly turn out to be alot of calculations and statuses for a simple grid.\n",
    " - **Make our file static in order to have the needed context for the MDP solver**\n",
    "\n",
    "For small grids in examples like 3x4 size, this would be a challenge but for bigger ones, Reinforcement Learning almost definitely need to be adopted in order to manage the changing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2335576-98d0-42fe-8ad2-e480e75a7c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: 'Kernelspec' module not installed in the selected interpreter (/home/alex/.cache/pypoetry/virtualenvs/decision-theory-fire-evacuation-jEf1M96d-py3.12/bin/python).\n",
      "\u001b[1;31m Please re-install or update 'jupyter'.\n",
      "\u001b[1;31mInstall 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from enum import Enum  # Needed for Action enum\n",
    "from envs.constants import Config, Action  # Import config and Action enum\n",
    "from envs.grid import Grid  # To instantiate a template grid for map info\n",
    "import gymnasium as gym  # Import gymnasium to use your env in MDP\n",
    "from envs.ui.training_room import TrainingRoom\n",
    "from envs.constants import config\n",
    "\n",
    "# Define constants if not already imported or globally accessible\n",
    "# (though it's better to get them from config)\n",
    "# GRID_SIZE = config.grid_size # Example\n",
    "\n",
    "\n",
    "class FireEvacuationAgentMDP:\n",
    "    def __init__(self, seed=None):\n",
    "        \"\"\"\n",
    "        Initializes the FireEvacuationAgentMDP (the MDP solver).\n",
    "        This class pre-computes the optimal policy for a STATIC fire environment.\n",
    "        \"\"\"\n",
    "        self.np_random = np.random.RandomState(seed)\n",
    "\n",
    "        # Create a template grid instance to read map information (walls, static fire positions)\n",
    "        # This grid instance should *not* be used for live simulation steps.\n",
    "        # It's specifically for the MDP solver to build its P and R matrices.\n",
    "        # Pass static_mode=True to ensure the grid is initialized predictably.\n",
    "        # We also pass dummy initial positions here, they'll be overwritten when iterating states.\n",
    "        self.grid_template = Grid(\n",
    "            TrainingRoom(),\n",
    "            static_mode=True,\n",
    "            initial_agent_pos=np.array([0, 0]),  # Dummy\n",
    "            initial_target_pos=np.array([0, 0]),\n",
    "            np_random=self.np_random,\n",
    "        )  # Dummy\n",
    "\n",
    "        self.rows = config.grid_size\n",
    "        self.cols = config.grid_size\n",
    "\n",
    "        # Define the set of actions the MDP can take\n",
    "        self.actions = [\n",
    "            Action.UP,\n",
    "            Action.DOWN,\n",
    "            Action.LEFT,\n",
    "            Action.RIGHT,\n",
    "            Action.PUT_OUT_FIRE,  # Include put out fire as an action\n",
    "        ]\n",
    "        self.movement_actions = [Action.UP, Action.DOWN, Action.LEFT, Action.RIGHT]\n",
    "\n",
    "        # Determine all possible agent and target positions based on traversable tiles\n",
    "        self.possible_traversable_positions = []\n",
    "        for y in range(self.rows):\n",
    "            for x in range(self.cols):\n",
    "                if (\n",
    "                    self.grid_template.tiles[x][y]\n",
    "                    and self.grid_template.tiles[x][y].is_traversable\n",
    "                ):\n",
    "                    self.possible_traversable_positions.append((x, y))\n",
    "\n",
    "        # 1. Define All Possible States\n",
    "        # State: (agent_x, agent_y, target_x, target_y)\n",
    "        self.states = self._define_states()\n",
    "        self.num_states = len(self.states)\n",
    "        self.state_to_idx = {state: i for i, state in enumerate(self.states)}\n",
    "        self.idx_to_state = {i: state for i, state in enumerate(self.states)}\n",
    "\n",
    "        # 2. Build Transition Probabilities (P) and Reward Function (R)\n",
    "        self.P = self._build_transition_probabilities()  # P[s_idx, a_idx, s_prime_idx]\n",
    "        self.R = self._build_reward_function()  # R[s_idx, a_idx]\n",
    "\n",
    "        # 3. Initialize Value Function and Policy\n",
    "        self.value_function = np.zeros(self.num_states)\n",
    "        self.policy = np.zeros(\n",
    "            self.num_states, dtype=int\n",
    "        )  # Stores index of optimal action\n",
    "\n",
    "        print(\n",
    "            f\"MDP Initialized with {self.num_states} states and {len(self.actions)} actions.\"\n",
    "        )\n",
    "        print(\"Starting Value Iteration...\")\n",
    "        # 4. Run Value Iteration to compute the optimal policy\n",
    "        self.value_iteration(epsilon=1e-6)  # Using a small epsilon for convergence\n",
    "        print(\"Value Iteration complete. Optimal policy computed.\")\n",
    "\n",
    "    def _define_states(self) -> list[tuple]:\n",
    "        \"\"\"\n",
    "        Defines the state space as (agent_x, agent_y, target_x, target_y).\n",
    "        Only includes positions that are traversable.\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        for ax, ay in self.possible_traversable_positions:\n",
    "            for tx, ty in self.possible_traversable_positions:\n",
    "                states.append((ax, ay, tx, ty))\n",
    "        return states\n",
    "\n",
    "    def _build_transition_probabilities(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Builds the transition probability matrix P[s_idx, a_idx, s_prime_idx].\n",
    "        Since fire is static and moves are deterministic (if legal),\n",
    "        P will mostly contain 1.0 for the expected next state and 0.0 otherwise.\n",
    "        \"\"\"\n",
    "        P = np.zeros((self.num_states, len(self.actions), self.num_states))\n",
    "\n",
    "        # Temporarily create a FireFighterWorld instance in static mode\n",
    "        # to simulate transitions for the MDP.\n",
    "        # This allows us to use the same `step` logic as the environment.\n",
    "        # We will reset this environment for each state to simulate transitions.\n",
    "        # Use render_mode=None as we don't need rendering for building P and R.\n",
    "        env_simulator = gym.make(\"FireFighterWorld\", render_mode=None, static_mode=True)\n",
    "\n",
    "        for s_idx, (ax, ay, tx, ty) in enumerate(self.states):\n",
    "            # Reset the simulator environment to the current state (ax, ay, tx, ty)\n",
    "            # using the options parameter for initial positions.\n",
    "            observation, info = env_simulator.reset(\n",
    "                options={\n",
    "                    \"initial_agent_pos\": np.array([ax, ay]),\n",
    "                    \"initial_target_pos\": np.array([tx, ty]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Get the actual positions from the reset grid to ensure consistency\n",
    "            # (though with static_mode and initial_pos, it should match)\n",
    "            current_agent_pos = np.array(observation)\n",
    "            current_target_pos = np.array(info[\"target_pos\"])\n",
    "\n",
    "            for a_idx, action in enumerate(self.actions):\n",
    "                # Simulate the step using the environment's logic\n",
    "                # Need to reset the environment to the *current* state before each action simulation\n",
    "                # because `env_simulator.step` changes its internal state.\n",
    "                env_simulator.reset(\n",
    "                    options={\n",
    "                        \"initial_agent_pos\": current_agent_pos,\n",
    "                        \"initial_target_pos\": current_target_pos,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                next_observation, reward, terminated, truncated, next_info = (\n",
    "                    env_simulator.step(action.value)\n",
    "                )\n",
    "                next_agent_pos = next_observation[0]\n",
    "                next_target_pos = np.array(\n",
    "                    next_info[\"target_pos\"]\n",
    "                )  # Target position is static\n",
    "\n",
    "                next_s_tuple = (\n",
    "                    next_agent_pos[0],\n",
    "                    next_agent_pos[1],\n",
    "                    next_target_pos[0],\n",
    "                    next_target_pos[1],\n",
    "                )\n",
    "\n",
    "                if next_s_tuple in self.state_to_idx:\n",
    "                    next_s_idx = self.state_to_idx[next_s_tuple]\n",
    "                    P[s_idx, a_idx, next_s_idx] = 1.0  # Deterministic transition\n",
    "                else:\n",
    "                    # This case indicates an error in state definition or transition logic\n",
    "                    # For safety, make it transition to current state if somehow invalid.\n",
    "                    P[s_idx, a_idx, s_idx] = 1.0\n",
    "                    print(\n",
    "                        f\"Warning: Calculated next state {next_s_tuple} not in defined state space.\"\n",
    "                    )\n",
    "\n",
    "        env_simulator.close()  # Close the simulator environment\n",
    "        return P\n",
    "\n",
    "    def _build_reward_function(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Builds the reward function R[s_idx, a_idx].\n",
    "        Rewards are associated with taking an action from a state.\n",
    "        \"\"\"\n",
    "        R = np.zeros((self.num_states, len(self.actions)))\n",
    "\n",
    "        # Temporarily create a FireFighterWorld instance in static mode\n",
    "        env_simulator = gym.make(\"FireFighterWorld\", render_mode=None, static_mode=True)\n",
    "\n",
    "        for s_idx, (ax, ay, tx, ty) in enumerate(self.states):\n",
    "            current_agent_pos = np.array([ax, ay])\n",
    "            current_target_pos = np.array([tx, ty])\n",
    "\n",
    "            for a_idx, action in enumerate(self.actions):\n",
    "                # Reset the simulator environment to the current state (ax, ay, tx, ty)\n",
    "                env_simulator.reset(\n",
    "                    options={\n",
    "                        \"initial_agent_pos\": current_agent_pos,\n",
    "                        \"initial_target_pos\": current_target_pos,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Take the step to get the reward\n",
    "                _, reward, _, _, _ = env_simulator.step(action.value)\n",
    "                R[s_idx, a_idx] = reward\n",
    "\n",
    "        env_simulator.close()  # Close the simulator environment\n",
    "        return R\n",
    "\n",
    "    def value_iteration(self, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Performs Value Iteration to find the optimal value function and policy.\n",
    "        \"\"\"\n",
    "        V = np.copy(self.value_function)\n",
    "        gamma = config.discount_factor  # Get discount factor from config\n",
    "\n",
    "        iteration = 0\n",
    "        while True:\n",
    "            iteration += 1\n",
    "            V_new = np.zeros(self.num_states)\n",
    "            delta = 0\n",
    "\n",
    "            for s_idx in range(self.num_states):\n",
    "                q_values = np.zeros(len(self.actions))\n",
    "                for a_idx in range(len(self.actions)):\n",
    "                    # Calculate expected future reward based on P and V\n",
    "                    expected_future_reward = np.sum(self.P[s_idx, a_idx, :] * V)\n",
    "\n",
    "                    q_values[a_idx] = (\n",
    "                        self.R[s_idx, a_idx] + gamma * expected_future_reward\n",
    "                    )\n",
    "\n",
    "                # If a state has no valid actions leading anywhere (e.g., surrounded by walls/fire,\n",
    "                # and no valid moves/actions, though unlikely with current setup), handle this:\n",
    "                if q_values.size > 0:\n",
    "                    V_new[s_idx] = np.max(q_values)\n",
    "                    self.policy[s_idx] = np.argmax(q_values)\n",
    "                else:\n",
    "                    # Fallback for states with no meaningful Q-values (should ideally not happen)\n",
    "                    V_new[s_idx] = 0  # Or some large negative value\n",
    "                    self.policy[s_idx] = 0  # Default to first action (e.g., UP)\n",
    "\n",
    "                delta = max(delta, abs(V_new[s_idx] - V[s_idx]))\n",
    "\n",
    "            V = V_new\n",
    "            if delta < epsilon:\n",
    "                print(f\"Value Iteration converged in {iteration} iterations.\")\n",
    "                break\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Value Iteration: {iteration} iterations, Delta: {delta:.6f}\")\n",
    "\n",
    "        self.value_function = V\n",
    "        return self.policy, self.value_function\n",
    "\n",
    "    def get_optimal_action(\n",
    "        self, current_agent_pos: np.ndarray, current_target_pos: np.ndarray\n",
    "    ) -> Action:\n",
    "        \"\"\"\n",
    "        Given the current agent and target positions, returns the optimal action\n",
    "        from the pre-computed policy.\n",
    "        \"\"\"\n",
    "        current_state_tuple = (\n",
    "            current_agent_pos[0],\n",
    "            current_agent_pos[1],\n",
    "            current_target_pos[0],\n",
    "            current_target_pos[1],\n",
    "        )\n",
    "\n",
    "        if current_state_tuple not in self.state_to_idx:\n",
    "            # This should ideally not happen if your environment only generates states\n",
    "            # that are part of your defined state space.\n",
    "            print(\n",
    "                f\"Error: Current state {current_state_tuple} not found in MDP state space. Returning default action.\"\n",
    "            )\n",
    "            return Action.UP  # Fallback to a default action\n",
    "\n",
    "        s_idx = self.state_to_idx[current_state_tuple]\n",
    "        optimal_action_idx = self.policy[s_idx]\n",
    "        return self.actions[optimal_action_idx]  # Return the actual Action enum member\n",
    "\n",
    "    def get_value_function(self):\n",
    "        return self.value_function\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"FireEvacuationAgentMDP (Solver Ready)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "855b73c4-c069-4ec3-bb06-c58885d64ab0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in static fire mode: True\n",
      "Grid size: 6\n",
      "Reward for success: 1000\n",
      "Discount factor: 0.95\n",
      "Initializing MDP Solver...\n",
      "MDP Initialized with 676 states and 5 actions.\n",
      "Starting Value Iteration...\n",
      "Value Iteration: 100 iterations, Delta: 6.232136\n",
      "Value Iteration: 200 iterations, Delta: 0.036898\n",
      "Value Iteration: 300 iterations, Delta: 0.000218\n",
      "Value Iteration: 400 iterations, Delta: 0.000001\n",
      "Value Iteration converged in 406 iterations.\n",
      "Value Iteration complete. Optimal policy computed.\n",
      "Creating environment for visualization...\n",
      "Resetting environment to initial state: Agent at [0 5], Target at [0 0]\n",
      "Starting MDP simulation from Agent: [0 5], Target: [0 0]\n",
      "Step 1: Agent at [0 5], Target at [0 0]. Optimal Action: UP\n",
      "Step 2: Agent at [0 4], Target at [0 0]. Optimal Action: RIGHT\n",
      "Step 3: Agent at [1 4], Target at [0 0]. Optimal Action: RIGHT\n",
      "Step 4: Agent at [2 4], Target at [0 0]. Optimal Action: RIGHT\n",
      "Step 5: Agent at [3 4], Target at [0 0]. Optimal Action: RIGHT\n",
      "Step 6: Agent at [4 4], Target at [0 0]. Optimal Action: UP\n",
      "Step 7: Agent at [4 3], Target at [0 0]. Optimal Action: UP\n",
      "Step 8: Agent at [4 2], Target at [0 0]. Optimal Action: LEFT\n",
      "Step 9: Agent at [3 2], Target at [0 0]. Optimal Action: LEFT\n",
      "Step 10: Agent at [2 2], Target at [0 0]. Optimal Action: UP\n",
      "Step 11: Agent at [2 1], Target at [0 0]. Optimal Action: LEFT\n",
      "Step 12: Agent at [1 1], Target at [0 0]. Optimal Action: LEFT\n",
      "Step 13: Agent at [0 1], Target at [0 0]. Optimal Action: UP\n",
      "Simulation finished in 13 steps. Total Reward: 1012\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time  # Import time for sleep\n",
    "import sys\n",
    "import os\n",
    "from mdp.agent import FireEvacuationAgentMDP\n",
    "from envs.ui.sprites import load_srpite_map\n",
    "\n",
    "load_srpite_map()\n",
    "\n",
    "# Add the directory containing your 'envs' package to the Python path\n",
    "# Adjust this path if your notebook is not in the same directory as 'envs'\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "# Import your config and Action enum\n",
    "from envs.constants import config, Action\n",
    "\n",
    "# Import your FireFighterWorld environment class\n",
    "from envs.grid_world import FireFighterWorld\n",
    "\n",
    "gym.envs.registration.register(\n",
    "    id=\"FireFighterWorld-v0\",  # It's good practice to add a version\n",
    "    entry_point=\"envs:FireFighterWorld\",\n",
    "    # You might want to specify max_episode_steps here, e.g., if you have a time limit for the MDP\n",
    "    # max_episode_steps=100,\n",
    "    # Or keep it out if your loop handles truncation.\n",
    ")\n",
    "\n",
    "\n",
    "# Ensure your constants are set up for a static environment for MDP\n",
    "config.chance_of_catching_fire = 0  # Ensure no new fire appears\n",
    "config.chance_of_self_extinguish = 0  # Ensure existing fire doesn't extinguish randomly\n",
    "\n",
    "# Set static_fire_mode in config BEFORE initializing MDP solver or environment\n",
    "# This flag affects how Grid initializes (e.g., character placement based on mode)\n",
    "config.static_fire_mode = True\n",
    "\n",
    "print(f\"Running in static fire mode: {config.static_fire_mode}\")\n",
    "print(f\"Grid size: {config.grid_size}\")\n",
    "print(f\"Reward for success: {config.evacuation_success_reward}\")\n",
    "print(f\"Discount factor: {config.discount_factor}\")\n",
    "\n",
    "\n",
    "def run_mdp_simulation():\n",
    "    print(\"Initializing MDP Solver...\")\n",
    "    # Initialize the MDP solver (this will compute the policy)\n",
    "    # The MDP solver's internal `gym.make` calls will now find 'FireFighterWorld-v0'\n",
    "    mdp_solver = FireEvacuationAgentMDP(seed=42)\n",
    "\n",
    "    print(\"Creating environment for visualization...\")\n",
    "    # Create the environment in static mode for visualization\n",
    "    env = gym.make(\"FireFighterWorld-v0\", render_mode=\"human\", static_mode=True)\n",
    "\n",
    "    # Define an initial state for the simulation (e.g., agent at (0,0), target at (3,2))\n",
    "    # Make sure these positions are traversable in your grid setup.\n",
    "    # Note: Your Grid's `create_grid` will use these if `static_mode=True`.\n",
    "    initial_agent_pos = np.array([0, 5])\n",
    "    initial_target_pos = np.array(\n",
    "        [0, 0]\n",
    "    )  # Example, adjust as needed based on your walls/traversable tiles\n",
    "\n",
    "    print(\n",
    "        f\"Resetting environment to initial state: Agent at {initial_agent_pos}, Target at {initial_target_pos}\"\n",
    "    )\n",
    "    # Reset the environment to the specific initial state\n",
    "    observation, info = env.reset(\n",
    "        options={\n",
    "            \"initial_agent_pos\": initial_agent_pos,\n",
    "            \"initial_target_pos\": initial_target_pos,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # The observation from env.reset will be the agent's (x,y) if your _get_obs() is still like that.\n",
    "    # We need to explicitly get target_pos from info.\n",
    "    agent_current_pos = observation[0]\n",
    "    target_current_pos = np.array(info[\"target_pos\"])  # Get target pos from info dict\n",
    "\n",
    "    print(\n",
    "        f\"Starting MDP simulation from Agent: {agent_current_pos}, Target: {target_current_pos}\"\n",
    "    )\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    max_simulation_steps = 100  # Add a hard step limit to prevent infinite loops\n",
    "\n",
    "    while not done and step_count < max_simulation_steps:\n",
    "        step_count += 1\n",
    "\n",
    "        # Get the optimal action from the pre-computed MDP policy\n",
    "        # Ensure that `current_agent_pos` and `current_target_pos` are tuples for `get_optimal_action`\n",
    "        optimal_action = mdp_solver.get_optimal_action(\n",
    "            agent_current_pos, target_current_pos\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Step {step_count}: Agent at {agent_current_pos}, Target at {target_current_pos}. Optimal Action: {optimal_action.name}\"\n",
    "        )\n",
    "\n",
    "        # Take the optimal action in the environment.\n",
    "        # Ensure you are passing the Action enum value (integer) to env.step()\n",
    "        next_observation, reward, terminated, truncated, next_info = env.step(\n",
    "            optimal_action.value\n",
    "        )\n",
    "\n",
    "        # Update current positions from the environment's step output\n",
    "        agent_current_pos = next_observation[0]\n",
    "        target_current_pos = np.array(\n",
    "            next_info[\"target_pos\"]\n",
    "        )  # Target position is static\n",
    "\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Render the frame to show the action\n",
    "        env.render()\n",
    "        time.sleep(0.5)  # Slow down rendering for better visualization\n",
    "\n",
    "    print(f\"Simulation finished in {step_count} steps. Total Reward: {total_reward}\")\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_mdp_simulation()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"MDP simulation interrupted.\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c15daf-7edf-4990-b179-5525d9ec6fc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9ff33-37e8-4729-88ec-c070e097a034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b859f-ca81-45f1-8f7c-acd7aec703cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
